{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c000b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import torch\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.document_loaders import PyPDFLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "import streamlit as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6364bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device (CUDA if available, otherwise CPU)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define paths for models\n",
    "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "EMBEDDING_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "# Load TinyLlama model and tokenizer\n",
    "@st.cache_resource\n",
    "def load_llm():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    # Create a text generation pipeline\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.7,\n",
    "        top_p=0.95,\n",
    "        repetition_penalty=1.15\n",
    "    )\n",
    "    \n",
    "    # Create LangChain wrapper for HuggingFace pipeline\n",
    "    hf_pipe = HuggingFacePipeline(pipeline=pipe)\n",
    "    return hf_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c79299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load and process documents\n",
    "def process_documents(file_paths):\n",
    "    documents = []\n",
    "    for file_path in file_paths:\n",
    "        if file_path.endswith('.pdf'):\n",
    "            loader = PyPDFLoader(file_path)\n",
    "        elif file_path.endswith('.txt'):\n",
    "            loader = TextLoader(file_path)\n",
    "        else:\n",
    "            print(f\"Unsupported file type: {file_path}\")\n",
    "            continue\n",
    "        \n",
    "        documents.extend(loader.load())\n",
    "    \n",
    "    # Split documents into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        length_function=len\n",
    "    )\n",
    "    \n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(chunks)} chunks\")\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125c869e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create vector store from document chunks\n",
    "def create_vector_store(chunks):\n",
    "    # Initialize embedding model\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=EMBEDDING_MODEL_NAME,\n",
    "        model_kwargs={'device': device}\n",
    "    )\n",
    "    \n",
    "    # Create vector store from documents\n",
    "    vector_store = Chroma.from_documents(\n",
    "        documents=chunks,\n",
    "        embedding=embeddings,\n",
    "        persist_directory=\"./chroma_db\"\n",
    "    )\n",
    "    \n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0ce355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create RAG chain\n",
    "def create_rag_chain(vector_store):\n",
    "    llm = load_llm()\n",
    "    \n",
    "    # Create retrieval chain\n",
    "    retriever = vector_store.as_retriever(\n",
    "        search_type=\"similarity\",\n",
    "        search_kwargs={\"k\": 3}\n",
    "    )\n",
    "    \n",
    "    # Create QA chain\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "        return_source_documents=True\n",
    "    )\n",
    "    \n",
    "    return qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18ff0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this information to the app\n",
    "def about_rag():\n",
    "    st.sidebar.header(\"About RAG\")\n",
    "    with st.sidebar.expander(\"What is RAG?\"):\n",
    "        st.write(\"\"\"\n",
    "        **Retrieval-Augmented Generation (RAG)** combines retrieval systems with\n",
    "        text generation models. It works by:\n",
    "        \n",
    "        1. **Retrieving** relevant information from a knowledge base\n",
    "        2. **Augmenting** the prompt to the LLM with this information\n",
    "        3. **Generating** a response based on both the question and retrieved context\n",
    "        \n",
    "        This helps overcome LLM limitations by providing up-to-date and specific information.\n",
    "        \"\"\")\n",
    "    \n",
    "    with st.sidebar.expander(\"Why use embeddings?\"):\n",
    "        st.write(\"\"\"\n",
    "        **Embeddings** are numerical representations of text that capture semantic meaning.\n",
    "        \n",
    "        Without embeddings, we would need to use basic keyword matching which:\n",
    "        - Misses semantic similarities\n",
    "        - Cannot understand context\n",
    "        - Relies only on exact word matches\n",
    "        \n",
    "        Embeddings allow us to find documents that are conceptually similar to the query,\n",
    "        even if they use different words.\n",
    "        \"\"\")\n",
    "    \n",
    "    with st.sidebar.expander(\"How retrieval works\"):\n",
    "        st.write(\"\"\"\n",
    "        When you ask a question:\n",
    "        \n",
    "        1. Your question is converted to an embedding vector\n",
    "        2. This vector is compared to document chunk vectors using similarity metrics\n",
    "        3. The most similar chunks are retrieved\n",
    "        4. These chunks provide context for the LLM's response\n",
    "        \n",
    "        The retriever acts as the system's \"memory\" while the LLM acts as the \"reasoning engine\".\n",
    "        \"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23dc74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streamlit interface\n",
    "def main():\n",
    "    st.title(\"ðŸ“š RAG Chatbot with TinyLlama and LangChain\")\n",
    "    \n",
    "    st.sidebar.header(\"Upload Documents\")\n",
    "    uploaded_files = st.sidebar.file_uploader(\n",
    "        \"Upload PDF or TXT files\", \n",
    "        type=[\"pdf\", \"txt\"], \n",
    "        accept_multiple_files=True\n",
    "    )\n",
    "\n",
    "    about_rag()  # Add this line\n",
    "    \n",
    "    # Process uploaded files\n",
    "    if uploaded_files:\n",
    "        with st.sidebar:\n",
    "            with st.spinner(\"Processing documents...\"):\n",
    "                # Save uploaded files temporarily\n",
    "                temp_file_paths = []\n",
    "                for file in uploaded_files:\n",
    "                    file_path = f\"temp_{file.name}\"\n",
    "                    with open(file_path, \"wb\") as f:\n",
    "                        f.write(file.getvalue())\n",
    "                    temp_file_paths.append(file_path)\n",
    "                \n",
    "                # Process documents\n",
    "                chunks = process_documents(temp_file_paths)\n",
    "                \n",
    "                # Create vector store\n",
    "                vector_store = create_vector_store(chunks)\n",
    "                \n",
    "                # Create QA chain\n",
    "                st.session_state.qa_chain = create_rag_chain(vector_store)\n",
    "                \n",
    "                st.success(f\"Processed {len(chunks)} document chunks!\")\n",
    "                \n",
    "                # Clean up temporary files\n",
    "                for file_path in temp_file_paths:\n",
    "                    if os.path.exists(file_path):\n",
    "                        os.remove(file_path)\n",
    "    \n",
    "    # Initialize chat history\n",
    "    if \"messages\" not in st.session_state:\n",
    "        st.session_state.messages = []\n",
    "    \n",
    "    # Display chat history\n",
    "    for message in st.session_state.messages:\n",
    "        with st.chat_message(message[\"role\"]):\n",
    "            st.write(message[\"content\"])\n",
    "    \n",
    "    # Chat input\n",
    "    if prompt := st.chat_input(\"Ask a question about your documents\"):\n",
    "        # Add user message to chat history\n",
    "        st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "        \n",
    "        # Display user message\n",
    "        with st.chat_message(\"user\"):\n",
    "            st.write(prompt)\n",
    "        \n",
    "        # Check if QA chain exists\n",
    "        if \"qa_chain\" not in st.session_state:\n",
    "            with st.chat_message(\"assistant\"):\n",
    "                st.write(\"Please upload documents first!\")\n",
    "                st.session_state.messages.append({\"role\": \"assistant\", \"content\": \"Please upload documents first!\"})\n",
    "        else:\n",
    "            # Generate response\n",
    "            with st.chat_message(\"assistant\"):\n",
    "                with st.spinner(\"Thinking...\"):\n",
    "                    response = st.session_state.qa_chain({\"query\": prompt})\n",
    "                    answer = response[\"result\"]\n",
    "                    sources = response[\"source_documents\"]\n",
    "                    \n",
    "                    st.write(answer)\n",
    "                    \n",
    "                    with st.expander(\"View Sources\"):\n",
    "                        for i, source in enumerate(sources):\n",
    "                            st.write(f\"Source {i+1}:\")\n",
    "                            st.write(source.page_content)\n",
    "                            st.write(\"---\")\n",
    "                    \n",
    "                    # Add assistant response to chat history\n",
    "                    st.session_state.messages.append({\"role\": \"assistant\", \"content\": answer})\n",
    "        \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "joy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
