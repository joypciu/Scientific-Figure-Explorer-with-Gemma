{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beginner-Friendly RAG Document Question-Answering System\n",
    "\n",
    "This notebook creates a Retrieval-Augmented Generation (RAG) system to answer questions based on documents (PDFs or text files). It’s designed to run on a CPU (no GPU required) and uses lightweight models to fit within 32GB RAM.\n",
    "\n",
    "## What You'll Learn\n",
    "- How to set up a Python environment for machine learning.\n",
    "- How to load and process documents (PDFs or text files).\n",
    "- How to create a vector store for document retrieval.\n",
    "- How to use a language model to answer questions based on documents.\n",
    "\n",
    "## Hardware\n",
    "- Your system: Intel i7-12700, 32GB RAM, no NVIDIA GPU (CPU-only).\n",
    "\n",
    "## Prerequisites\n",
    "- Basic Python knowledge.\n",
    "- A few sample PDF or text files to use as input.\n",
    "\n",
    "Let’s get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Required Libraries\n",
    "\n",
    "Run the cell below to install the necessary Python packages. This may take a few minutes.\n",
    "\n",
    "**Note**: If you’re running this on your local machine, you may want to create a virtual environment first:\n",
    "```bash\n",
    "python -m venv rag_env\n",
    "source rag_env/bin/activate  # On Windows: rag_env\\Scripts\\activate\n",
    "```\n",
    "Then run the pip install command below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch transformers sentence-transformers langchain langchain-huggingface chromadb pypdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Import Libraries and Set Up the Environment\n",
    "\n",
    "We’ll import the required libraries and ensure the system uses your CPU. Since you don’t have an NVIDIA GPU, we’ll configure the models to run on CPU with `torch.float32` to avoid memory issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain.document_loaders import PyPDFLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_huggingface import HuggingFacePipeline, HuggingFaceEmbeddings\n",
    "import streamlit as st  # Used for caching, but we’ll simulate it here\n",
    "\n",
    "# Set device to CPU (no GPU available)\n",
    "device = \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define model names\n",
    "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  # Lightweight model for CPU\n",
    "EMBEDDING_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"  # Lightweight embedding model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load the Language Model\n",
    "\n",
    "We’ll load the TinyLlama model, which is small enough to run on your system. The `@st.cache_resource` decorator is used in Streamlit apps to cache the model, but in a Jupyter Notebook, we’ll load it directly.\n",
    "\n",
    "**What’s happening here?**\n",
    "- **Tokenizer**: Converts text to numbers the model understands.\n",
    "- **Model**: The TinyLlama language model generates text.\n",
    "- **Pipeline**: A tool to generate text easily with controlled parameters (e.g., max tokens, temperature).\n",
    "\n",
    "**Note**: This step may take a few minutes to download the model (about 2GB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_llm():\n",
    "    # Load tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        torch_dtype=torch.float32,  # Use float32 for CPU compatibility\n",
    "        device_map=\"cpu\",  # Explicitly map to CPU\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    # Create a text generation pipeline\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_new_tokens=256,  # Limit response length\n",
    "        temperature=0.7,  # Controls randomness\n",
    "        top_p=0.95,  # Controls diversity\n",
    "        repetition_penalty=1.15,  # Avoids repetitive text \n",
    "        do_sample=True, \n",
    "        top_k=50,\n",
    "    )\n",
    "    \n",
    "    # Wrap pipeline in LangChain\n",
    "    hf_pipe = HuggingFacePipeline(pipeline=pipe)\n",
    "    return hf_pipe\n",
    "\n",
    "# Load the model (run this once)\n",
    "llm = load_llm()\n",
    "print(\"Language model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Process Documents\n",
    "\n",
    "This function loads and processes your documents (PDFs or text files). You’ll need to specify the file paths.\n",
    "\n",
    "**What’s happening?**\n",
    "- **Loaders**: `PyPDFLoader` for PDFs, `TextLoader` for text files.\n",
    "- **Text Splitter**: Breaks documents into smaller chunks (1000 characters each) to make retrieval efficient.\n",
    "\n",
    "**Action**: Replace `file_paths` with the paths to your PDF or text files (e.g., `[\"./document.pdf\", \"./notes.txt\"]`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_documents(file_paths):\n",
    "    documents = []\n",
    "    for file_path in file_paths:\n",
    "        if file_path.endswith('.pdf'):\n",
    "            loader = PyPDFLoader(file_path)\n",
    "        elif file_path.endswith('.txt'):\n",
    "            loader = TextLoader(file_path)\n",
    "        else:\n",
    "            print(f\"Unsupported file type: {file_path}\")\n",
    "            continue\n",
    "        \n",
    "        documents.extend(loader.load())\n",
    "    \n",
    "    # Split documents into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        length_function=len\n",
    "    )\n",
    "    \n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(chunks)} chunks\")\n",
    "    return chunks\n",
    "\n",
    "# Specify your document paths here\n",
    "file_paths = [\"./Junior Python Developer (AI-focused).pdf\"]  # Replace with your file paths\n",
    "chunks = process_documents(file_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create a Vector Store\n",
    "\n",
    "We’ll convert document chunks into numerical embeddings and store them in a vector database (Chroma) for fast retrieval.\n",
    "\n",
    "**What’s happening?**\n",
    "- **Embeddings**: Convert text chunks into vectors using a lightweight model (`all-MiniLM-L6-v2`).\n",
    "- **Vector Store**: Stores embeddings for similarity-based retrieval.\n",
    "\n",
    "This step creates a `chroma_db` folder in your working directory to persist the vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector_store(chunks):\n",
    "    # Initialize embedding model\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=EMBEDDING_MODEL_NAME,\n",
    "        model_kwargs={'device': device}\n",
    "    )\n",
    "    \n",
    "    # Create vector store\n",
    "    vector_store = Chroma.from_documents(\n",
    "        documents=chunks,\n",
    "        embedding=embeddings,\n",
    "        persist_directory=\"./chroma_db\"\n",
    "    )\n",
    "    \n",
    "    return vector_store\n",
    "\n",
    "# Create the vector store\n",
    "vector_store = create_vector_store(chunks)\n",
    "print(\"Vector store created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Create the RAG Chain\n",
    "\n",
    "This combines the language model and vector store to answer questions based on your documents.\n",
    "\n",
    "**What’s happening?**\n",
    "- **Retriever**: Finds the top 3 most relevant document chunks for a query.\n",
    "- **QA Chain**: Uses the language model to generate answers based on retrieved chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rag_chain(vector_store):\n",
    "    # Create retriever\n",
    "    retriever = vector_store.as_retriever(\n",
    "        search_type=\"similarity\",\n",
    "        search_kwargs={\"k\": 3}\n",
    "    )\n",
    "    \n",
    "    # Create QA chain\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "        return_source_documents=True\n",
    "    )\n",
    "    \n",
    "    return qa_chain\n",
    "\n",
    "# Create the RAG chain\n",
    "qa_chain = create_rag_chain(vector_store)\n",
    "print(\"RAG chain created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Ask Questions\n",
    "\n",
    "Now you can ask questions about your documents! The system will retrieve relevant chunks and generate an answer.\n",
    "\n",
    "**Example**: If your document is about space exploration, you could ask, \"What is the purpose of the Mars Rover?\"\n",
    "\n",
    "Run the cell below and replace the `query` with your question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask a question\n",
    "query = \"summerize the document\"  # Replace with your question\n",
    "result = qa_chain({\"query\": query})\n",
    "\n",
    "# Print the answer and source documents\n",
    "print(\"Answer:\", result[\"result\"])\n",
    "print(\"\\nSource Documents:\")\n",
    "for doc in result[\"source_documents\"]:\n",
    "    print(f\"- {doc.metadata['source']}: {doc.page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: What’s Next?\n",
    "\n",
    "- **Experiment**: Try different questions or add more documents.\n",
    "- **Optimize**: Adjust `chunk_size`, `chunk_overlap`, or `max_new_tokens` to improve results.\n",
    "- **Learn More**: Check out the [LangChain documentation](https://python.langchain.com/docs/get_started/introduction) or [Hugging Face tutorials](https://huggingface.co/docs/transformers/index).\n",
    "\n",
    "If you run into memory issues, try reducing `max_new_tokens` or using smaller document chunks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "joy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
